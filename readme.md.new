# Projeto ETL de Rendimento AgrÃ­cola 

Este projeto implementa um **pipeline ETL** que processa dados de rendimento de safras agrÃ­colas, com possibilidade de integraÃ§Ã£o de dados climÃ¡ticos, e carrega os resultados em um **banco de dados PostgreSQL**. O objetivo Ã© permitir anÃ¡lises histÃ³ricas e correlaÃ§Ã£o entre rendimento e variÃ¡veis ambientais.

---

## Tecnologias utilizadas

| Categoria | Ferramenta | Uso no Projeto |
| :--- | :--- | :--- |
| **Pipeline** | **Python 3** | OrquestraÃ§Ã£o, LÃ³gica ETL e IntegraÃ§Ã£o com API (`requests`). |
| **TransformaÃ§Ã£o** | **Pandas / NumPy** | Limpeza, padronizaÃ§Ã£o e enriquecimento de dados. |
| **Data Warehouse** | **PostgreSQL** | Armazenamento relacional final (`yield_records` e `current_weather_data`). |
| **Conectividade** | **SQLAlchemy / `psycopg2`** | ConexÃ£o segura entre Python e PostgreSQL. |
| **Ambiente** | **Docker / Docker Compose** | ContainerizaÃ§Ã£o completa do DB, ETL e Ambiente de AnÃ¡lise. |
| **AnÃ¡lise** | **Jupyter Notebook** | AnÃ¡lise ExploratÃ³ria de Dados (EDA) e VisualizaÃ§Ã£o. |
| **IntegraÃ§Ã£o** | **OpenWeatherMap API** | Fonte de dados climÃ¡ticos em tempo real. |

---

##  Estrutura do projeto

```
AGRO_ETL/
â”‚â”€â”€ data/
â”‚   â””â”€â”€ raw/
â”‚       â””â”€â”€ yield_data.csv        # Dados histÃ³ricos de rendimento (Input primÃ¡rio)
â”‚â”€â”€ notebooks/
â”‚   â””â”€â”€ analysis.ipynb           # Script de conexÃ£o e AnÃ¡lise ExploratÃ³ria (EDA)
â”‚â”€â”€ .env.example                 # Exemplo de variÃ¡veis de ambiente (Credenciais e API Key)
â”‚â”€â”€ .env                         # Arquivo com as credenciais reais (ignorado pelo Git)
â”‚â”€â”€ docker-compose.yml           # Define os serviÃ§os 'postgres', 'pgadmin' e 'etl_runner'
â”‚â”€â”€ Dockerfile                   # ConstrÃ³i o ambiente Python/Jupyter/Pandas
â”‚â”€â”€ schema.sql                   # Script para criar tabelas no PostgreSQL
â”‚â”€â”€ requirements.txt             # DependÃªncias Python
â”‚â”€â”€ ETL_pipeline.py             # O script principal do pipeline (ExtraÃ§Ã£o API + Load)
â”‚â”€â”€ README.md
```

---

## ğŸš€ Como Rodar o Projeto

### ğŸ§© PrÃ©-requisitos
Certifique-se de ter o **Docker Desktop** instalado, em execuÃ§Ã£o, e o **Git** configurado.

---

### 1. ğŸ› ï¸ PreparaÃ§Ã£o e Credenciais
VocÃª deve criar o CSV de dados primÃ¡rios e obter a chave de API.

1. **Crie a estrutura `data/raw/`** e insira o arquivo `yield_data.csv`  
   (com as colunas: `crop`, `year`, `state`, `yield_kg_ha`, etc.).

2. **Obtenha uma Chave de API** gratuita na [OpenWeatherMap](https://openweathermap.org/api).

3. **Copie o arquivo de exemplo e preencha as credenciais**, incluindo sua `OPENWEATHER_API_KEY`:

```bash
cp .env.example .env
# Edite o arquivo .env com seus valores
```

### 2. ğŸ³ Subindo a Infraestrutura (Docker Compose)
Este comando constrÃ³i a imagem Python, baixa o PostgreSQL e inicia todos os containers em segundo plano.

```bash
docker-compose up -d
```

### 3. ğŸ”„ Executando o Pipeline ETL
Com os containers ativos, execute o script Python.
Ele irÃ¡ ler o CSV, buscar dados na API (apÃ³s a chave ser ativada) e carregar no PostgreSQL:

```bash
docker exec agro_etl_runner python ETL_pipeline.py
```

### 4. ğŸ“Š AnÃ¡lise e ValidaÃ§Ã£o (Jupyter Notebook)
Acesse o ambiente de anÃ¡lise para visualizar os dados e validar o pipeline.

URL: http://localhost:8888

Abra o arquivo `notebooks/analysis.ipynb` e execute as cÃ©lulas de consulta/visualizaÃ§Ã£o.

### 5. ğŸ—„ï¸ (Opcional) Acesso ao Data Warehouse (pgAdmin)
Para inspecionar as tabelas carregadas:

URL: http://localhost:8080

Login: admin@admin.com  
Senha: admin

Conecte-se ao servidor PostgreSQL usando as credenciais do seu arquivo `.env`.

## ğŸ“ˆ Status do Projeto
Funcional e em Desenvolvimento â€“ A integraÃ§Ã£o com a API estÃ¡ no lugar; o prÃ³ximo passo Ã© a OrquestraÃ§Ã£o (Airflow/Prefect).

## ğŸ‘¨â€ğŸ’» Autor
Mateus Bastos [@MateuSansete](https://github.com/MateuSansete)