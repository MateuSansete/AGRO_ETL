# Projeto ETL de Rendimento Agrícola 

Este projeto implementa um **pipeline ETL** que processa dados de rendimento de safras agrícolas, com possibilidade de integração de dados climáticos, e carrega os resultados em um **banco de dados PostgreSQL**. O objetivo é permitir análises históricas e correlação entre rendimento e variáveis ambientais.

---

## Tecnologias utilizadas

| Categoria | Ferramenta | Uso no Projeto |
| :--- | :--- | :--- |
| **Pipeline** | **Python 3** | Orquestração, Lógica ETL e Integração com API (`requests`). |
| **Transformação** | **Pandas / NumPy** | Limpeza, padronização e enriquecimento de dados. |
| **Data Warehouse** | **PostgreSQL** | Armazenamento relacional final (`yield_records` e `current_weather_data`). |
| **Conectividade** | **SQLAlchemy / `psycopg2`** | Conexão segura entre Python e PostgreSQL. |
| **Ambiente** | **Docker / Docker Compose** | Containerização completa do DB, ETL e Ambiente de Análise. |
| **Análise** | **Jupyter Notebook** | Análise Exploratória de Dados (EDA) e Visualização. |
| **Integração** | **OpenWeatherMap API** | Fonte de dados climáticos em tempo real. |

---

##  Estrutura do projeto

```
AGRO_ETL/
│── data/
│   └── raw/
│       └── yield_data.csv        # Dados históricos de rendimento (Input primário)
│── notebooks/
│   └── analysis.ipynb           # Script de conexão e Análise Exploratória (EDA)
│── .env.example                 # Exemplo de variáveis de ambiente (Credenciais e API Key)
│── .env                         # Arquivo com as credenciais reais (ignorado pelo Git)
│── docker-compose.yml           # Define os serviços 'postgres', 'pgadmin' e 'etl_runner'
│── Dockerfile                   # Constrói o ambiente Python/Jupyter/Pandas
│── schema.sql                   # Script para criar tabelas no PostgreSQL
│── requirements.txt             # Dependências Python
│── ETL_pipeline.py             # O script principal do pipeline (Extração API + Load)
│── README.md
```

---

## 🚀 Como Rodar o Projeto

### 🧩 Pré-requisitos
Certifique-se de ter o **Docker Desktop** instalado, em execução, e o **Git** configurado.

---

### 1. 🛠️ Preparação e Credenciais
Você deve criar o CSV de dados primários e obter a chave de API.

1. **Crie a estrutura `data/raw/`** e insira o arquivo `yield_data.csv`  
   (com as colunas: `crop`, `year`, `state`, `yield_kg_ha`, etc.).

2. **Obtenha uma Chave de API** gratuita na [OpenWeatherMap](https://openweathermap.org/api).

3. **Copie o arquivo de exemplo e preencha as credenciais**, incluindo sua `OPENWEATHER_API_KEY`:

```bash
cp .env.example .env
# Edite o arquivo .env com seus valores
```

### 2. 🐳 Subindo a Infraestrutura (Docker Compose)
Este comando constrói a imagem Python, baixa o PostgreSQL e inicia todos os containers em segundo plano.

```bash
docker-compose up -d
```

### 3. 🔄 Executando o Pipeline ETL
Com os containers ativos, execute o script Python.
Ele irá ler o CSV, buscar dados na API (após a chave ser ativada) e carregar no PostgreSQL:

```bash
docker exec agro_etl_runner python ETL_pipeline.py
```

### 4. 📊 Análise e Validação (Jupyter Notebook)
Acesse o ambiente de análise para visualizar os dados e validar o pipeline.

URL: http://localhost:8888

Abra o arquivo `notebooks/analysis.ipynb` e execute as células de consulta/visualização.

### 5. 🗄️ (Opcional) Acesso ao Data Warehouse (pgAdmin)
Para inspecionar as tabelas carregadas:

URL: http://localhost:8080

Login: admin@admin.com  
Senha: admin

Conecte-se ao servidor PostgreSQL usando as credenciais do seu arquivo `.env`.

## 📈 Status do Projeto
Funcional e em Desenvolvimento – A integração com a API está no lugar; o próximo passo é a Orquestração (Airflow/Prefect).

## 👨‍💻 Autor
Mateus Bastos [@MateuSansete](https://github.com/MateuSansete)